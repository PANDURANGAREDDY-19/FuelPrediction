{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from feature_engineering import engineer_features, prepare_data\n",
    "from model import train_models\n",
    "from evaluation import compare_models, evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/fuel_data.csv')\n",
    "df = engineer_features(df)\n",
    "X, y, scaler = prepare_data(df)\n",
    "\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {X.shape[0]}\")\n",
    "print(f\"\\nFeature Names:\\n{list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models, X_train, X_test, y_train, y_test = train_models(X, y)\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare_models(trained_models, X_test, y_test)\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nðŸ“Š MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (name, model) in enumerate(trained_models.items()):\n",
    "    y_pred = model.predict(X_test)\n",
    "    axes[idx].scatter(y_test, y_pred, alpha=0.5)\n",
    "    axes[idx].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    axes[idx].set_xlabel('Actual Fuel (L)')\n",
    "    axes[idx].set_ylabel('Predicted Fuel (L)')\n",
    "    axes[idx].set_title(f'{name}\\nRÂ² = {results[name][\"R2\"]:.4f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Only: Actual vs Predicted Fuel Consumption\n",
    "xgb_model = trained_models['XGBoost']\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(y_test, y_pred_xgb, alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Fuel Consumption (Liters)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Predicted Fuel Consumption (Liters)', fontsize=14, fontweight='bold')\n",
    "plt.title(f'XGBoost Model: Actual vs Predicted\\nRÂ² = {results[\"XGBoost\"][\"R2\"]:.4f} | RMSE = {results[\"XGBoost\"][\"RMSE\"]:.2f} L', fontsize=16, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = trained_models['XGBoost']\n",
    "feature_names = X.columns\n",
    "importances = best_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(importances)), importances[indices])\n",
    "plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "plt.title('Feature Importance (XGBoost)')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "for i in range(5):\n",
    "    print(f\"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(y_pred, residuals, alpha=0.5)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0].set_xlabel('Predicted Fuel (L)')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "axes[0].set_title('Residual Plot')\n",
    "\n",
    "axes[1].hist(residuals, bins=30, edgecolor='black')\n",
    "axes[1].set_xlabel('Residuals')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Residual Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = results_df[['MAE', 'RMSE', 'R2']]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, metric in enumerate(['MAE', 'RMSE', 'R2']):\n",
    "    metrics_df[metric].plot(kind='bar', ax=axes[idx], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "    axes[idx].set_title(f'{metric} Comparison')\n",
    "    axes[idx].set_ylabel(metric)\n",
    "    axes[idx].set_xlabel('Model')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
